# Into-The-Metaverse
## 2022 HackIllinois
> Link to demo video: https://youtu.be/kHpNRVDTwXw
> Link to view our project package: https://drive.google.com/file/d/1zIYetS6Ub_e7R07XgU-we7dgRQJZDUm6/view?usp=sharing

![image](https://user-images.githubusercontent.com/80299116/218271000-c017f6c8-d9aa-4e5e-904f-7466d66a48af.png)


## How we built it
There are two types of NPC, stationary and moving. The stationary NPC explains what the player can do at the place when the player gets closer. The movable NPC walks around and waves its hand when the player gets closer to it. As we introduced various features into one space, we ensured that the player could get guidance from the NPCs that walk around and talk to you. When the player reaches out to the surrounding npc to try out one of our features, the npc gives a brief introduction to how it can be used.

The first application that is introduced in our project is the live translator in virtual reality. Through the Azure speech-to-text application, this grabbable window frame allows the user to instantaneously convert their speech into text, with an additional feature of translating that text into multiple languages. The purpose of this application is to erase the boundary between the people due to language barriers and therefore make the metaverse open and interactive to anyone around the world.

The second application introduced is the background music-controlled via a virtual jukebox. The interface allows the user to change the music and the album corresponding to the music played. The background music audio is adjusted to be heard anywhere in the virtual world.

The third application is a VR keyboard with a Youtube player that works on a virtual screen. Through using Youtube API V3 users can type and search keywords which makes the video player to show a list of videos they searched.

The final application is a drawing board with 3 different interactive pens. The player can grab markers with different colors and draw lines onto the board, which was done by understanding the line rendering system in Unity.

## Challenges we ran into
This implementation of the virtual environment was done by each of the members working on individual features and merging them into one program. However, the majority of us didnâ€™t have Oculus Quest or devices to run the environment. Thus, being unable to test each of our features individually within the virtual environment was a huge challenge.

## What we learned
Through this Hackathon, we have developed teamwork skillsets and further deepened our knowledge in unity. The algorithms we have learned and implemented were stationary and movable NPCs, video surfing platform, jukebox, drawable whiteboard, and language translation.

## Built With 
Unity, C#, Oculus Quest, Meta Quest, VRTK

![image](https://user-images.githubusercontent.com/80299116/218271022-70c6335c-846f-4e2b-8932-be6a0a615128.png)


